\section{Goals}
The goal of this analysis is the classification of neutrino measurements in data from Monte Carlo the IceCube experiment. 
First, the minimum redundancy, maximum relevance (mRMR) selection is used to determine the best features.
This is directly followed by the application and comparison of three different machine learning algorithms for the classification.


\section{Theory}
\label{sec:Theory}
Before starting the data analysis, it is important to investigate the origin of the measured particles and understand the applied algorithms. 

\subsection{Cosmic Rays}
So-called "Cosmic Rays" consist of highly energetic particles, for examples protons, electrons, different nuclei as well as neutrinos. 
The study of these particles has been a point of interest for many centuries but the therm cosmic "Cosmic rays" was established in the early 20-th century. 
Incoming cosmic rays usually interact with the matter in the earths atmosphere creating particle showers. 
These incoming particles originate from a variety of different sources in the cosmos, some examples are active galactic nuclei or supernovae.
Their energy spectrum is described by
\begin{equation*}
	\frac{\mathrm{d}\Phi}{\mathrm{d}E} = \Phi_0 E^\gamma,
\end{equation*}
where $\gamma \approx -2.7$ is the so-called spectral index. 
The energy carried by these particles can range up to $10^{20}\,\unit{\eV}$.


\subsection{Atmospheric and astrophysical leptons}
The showers caused by cosmic rays create a range of different particles. 
High energetic muons and neutrinos originate mostly from the decay of lighter mesons in atmospheric particle showers. 
In the IceCube experiment these are referred to as \textit{conventional} and form a large background in the data.
As the lighter mesons loose some of their energy the resulting energy spectrum for the muons 
and neutrinos is lower and follows a $E^{-3.7}$ proportionality. 
If the muons and neutrinos originate from heavier hadrons 
(e.g $\Lambda$-baryons or $D$-mesons), their energy spectrum will much more resemble those from astrophysical ones. 
This is caused by the short lifetime of the heavier hadrons, leaving them less time to loose energy.
When a particle originating from the decay of a short lived particle is detected it is called \textit{prompt}. From the prompt particles the signal are those which originate form the decay of a neutrino.
%When an astrophysical particle is detected at the IceCube experiment, it is called \textit{prompt} and is
%usually the signal that has to be extracted from the large background.

\subsection{mRMR Selection}
\label{sec:mRmR}
The mRMR selection algorithm is implemented using python library \texttt{mrmr-selection}. This algorithm works by maximizing the correlation to the target and tries to minimize the correlation between the individual features. This is achieved using the joint information 
\begin{equation*}
	I(x,y) = \int p(x,y)\log\left(\frac{p(x,y)}{p(x)p(y)}\right)\mathrm{d}x\mathrm{d}y,
\end{equation*}
where $p(x),p(y),p(x,y)$ are the probability densities of the different features. It is a feature selection algorithm independent of the type of learner applied.

\subsection{Naive Bayes Classifier}
The Naive Bayes Classifier uses a Bayesian approach in order to classify candidates as signal $A$ or background $\bar{A}$. It assumes the probability $p(B|A)$ to be background/signal with the given feature $V$ is only dependent on $B$ (naive). The classifier calculates
\begin{equation*}
	Q = \prod_{i=1}^{n} \frac{p(B_i|A)}{p(B_i|\bar{A})},
\end{equation*}
which is $> 1$ for a signal and $< 1$ for a background prediction.

\subsection{Random Forest Classifier}
A random forest classifier uses $k$ binary decision trees, which try to determine ideal cuts on the given features by minimizing a gini-impurity function
\begin{equation}
	\mathrm{Gini}(p_i) = 1 - \sum_{i=1}^{k}{p_i}^1,
\end{equation}
where $p_i$ is the probability of a candidate belonging to a class $i$.
Each tree is trained on a random subset of the data in order to avoid over training. 
The determined result of each tree is then averaged or the majority voting is used depending on the implementation. 

\subsection{Multi-Layer Perceptron Classifier}
The Multi-Layer Perceptron Classifier (MLPClassifier) is a classic neural network with Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm (LBFGS) loss function \cite{Lossfueicecube}. It is trained using labeled data. As the MLPClassifier usually is implemented with multiple hidden layers it allows classification of non linear problems.

\subsection{Evaluation of the Models}
All the above mentioned models are evaluated using a range of metrics. During the classification, a subsample of the training data is used to check the models for overtraining. This procedure is further described in \autoref{Hier label}.%This procedure is called cross validation and it can contain for example crosschecks of the loss function for training and validation data. 
After the training the accuracy $a$ and the precision $p$ are determined on a test data sample using 
\begin{align*}
	a =& \frac{\mathrm{TP + TN}}{\mathrm{TP + TN + FP + FN}},\\
	p =&  \frac{\mathrm{TP}}{\mathrm{TP + FP}},\\
	r =& \frac{\mathrm{TP}}{\mathrm{TP + FN}},
\end{align*}
where TP, TN, FP and FN are the counts of correctly classified signal (TP) and background (TN) and respectively falsely classified signal (FP) and background (FN). Ideally both values are close to 1. 
Additionally , the $f_\beta$ score
\begin{equation}
	f_\beta = (1+\beta^2) \frac{p\cdot r}{\beta^2p+r}
\end{equation}
is calculated using the Recall $r$.
By using a $\beta \neq 1$, it is possible to give a higher importance to the precision $p$ or recall $r$, respectively.
Lastly the Receiver Operating Characteristic (ROC) curve is determined by plotting the True Positive Rate 
\begin{equation*}
	\mathrm{TPR} = \frac{\mathrm{TP}}{\mathrm{TP + FN}}
\end{equation*}
against the False Positive Rate 
\begin{equation*}
	\mathrm{TPR} = \frac{\mathrm{FP}}{\mathrm{FP + TN}}
\end{equation*}
for different cut values $\tau_c$ on the prediction of the classifier. A metric for the quality of the classification is the area under the aforementioned curve. A value of 0.5 results from random guessing whereas a value of 1 is ideal.









