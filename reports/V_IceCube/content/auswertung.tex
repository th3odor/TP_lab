\section{Data analysis}
\label{sec:Auswertung}

In total, three different data sets are available for this analysis. 
The training of the classifiers will be performed using the two datasets
\texttt{signal\_train.csv} and \texttt{background\_train.csv}, while the
\texttt{test.csv} file will be used to test the created model.

\subsection{Data preprocessing}

The events in the signal and background training datasets were produced via Monte Carlo simulations.
Consequently, the datasets contain Monte Carlo truths, event ids and weights which should not be
used during the training process, as they are not present in actual data.
All attributes with a name containing the key words \texttt{Weight}, \texttt{MC}, \texttt{Corsika}, or
\texttt{I3EventHeader} are removed from the datasets.
The training data is already labeled with $0$ for background and $1$ for signal. These labels are removed and 
stored, as they should only be used for validation purposes. Some entries contain \texttt{Nans} or
\texttt{Infs}, which have to be removed. As a first step, all attributes where more than \qty{10}{\percent}
of entries are \texttt{Nans} or \texttt{Infs} are removed. The remaining invalid values are adressed by
deleting all events where any attribute is \texttt{Nan} or \texttt{Inf}.
Lastly, the data is rescaled by removing the mean and scaling to unit variance.

\subsection{Attribute selection}
\label{sec:att_selection}

Before the multivariate section can be performed, a selection of suitable attributes is needed, which the classifier
can use. The goal is to choose attributes which contain the most information about wether an event
is signal or background. Using all available attributes is avoided as this can lead to overtraining and
increases computational time.
The selection is performed using the mRMR method described in \autoref{sec:mRmR}. Using the package
\texttt{mrmr\_selection} \cite{mrmr}, the $100$ best features are determined in ascending order.
In \autoref{fig:features}, the distributions of the four best features for signal and background events
are shown.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{build/features.pdf}
  \caption{Distributions of the four best features determined via the mRMR method, for signal and background events.}
  \label{fig:features}
\end{figure}

All distributions show a good seperation of signal and background, which is excpected
from the best features determined by the mRMR method.

\subsection{Multivariate selection}

In this section, three different models are trained and compared. All models are implemented from the \texttt{scikit-learn}
libary \cite{scikit-learn}.

\subsubsection{Naives Bayes}

The first model used is the Naives Bayes classifier. To determine the optimal amount of
features, the model is trained and tested multiple times with different numbers of attributes.
For each case, the best attributes determined in \autoref{sec:att_selection} are used, e.g. at first 
only the best five features are selected, then the best ten features and so on.
Quality parameters are computed for each case and plotted against the number of features in \autoref{fig:NaiveBayes_features}.
The accuracy, precision and ROC score are chosen as the quality parameters for all three
models tested in this analysis.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{build/Naive_Bayes/Naive_Bayes_features.pdf}
  \caption{Quality parameters of the Naive Bayes classifier for a varying number of features.}
  \label{fig:NaiveBayes_features}
\end{figure}

Since the Naive Bayes is a very simple model, adding more then five features only leads to random
fluctuations in the model performance. This is excpected, as the model has a low capacity and
cannot make use of the additional information provided when more features are given.
This is why only the five best attributes are used to train this classifier.
To validate the classifiers performance, a k-folding algorithm is used with five splits.
This means that the data is split in five parts, the classifier is then trained on four parts and tested using
the remaining part. In this fashion, five classifiers are each trained and tested on different subsets.
If the model is working good the results should be similar for all choices of subsets.
The scores of the Naive Bayes classifier are determined to be %vllt ein halber satz zur abweichung
\begin{align*}
  \text{Accuracy} &= 0.8432\pm0.0029 \,, \\
  \text{Precision} &= 0.9074\pm0.0019 \,, \\
  \text{ROC Score} &= 0.9278\pm0.0013 \,.
\end{align*}
The optimal decision threshold for the predicted probabilities is calculated using
the $f_\beta$ score. Here, $\beta=0.1$ is used, meaning that much more importance
is placed on the precision of the classification rather then the accuracy.
The $f_\beta$ score is computed for each possible threshold $\tau_c$, then
the maximum value is determined and consequently the best value for $\tau_c$.
The score and its maximum are displayed in \autoref{fig:NaiveBayes_threshold}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{build/Naive_Bayes/Naive_Bayes_threshold.pdf}
  \caption{The $f_\beta$ score of the Naive Bayes classifier and its maximum value.}
  \label{fig:NaiveBayes_threshold}
\end{figure}

At the maximum, the best value of the cut $\tau_{c \text{, best}}$ is also given in \autoref{fig:NaiveBayes_threshold}.
After applying the determined cut on the predicted probabilities of the Naive Bayes model,
a confusion matrix shown in \autoref{fig:NaiveBayes_confusion} is created.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{build/Naive_Bayes/Naive_Bayes_confusion.pdf}
  \caption{Confusion matrix for the Naive Bayes classifier.}
  \label{fig:NaiveBayes_confusion}
\end{figure}

Due to the low capacity of the model and the high focus on precision, a lot of signal events are missed.

\subsubsection{Random Forest}

The steps performed for the Random Forest are identical to the ones from the Naive Bayes model.
The Random Forest is initialized with $100$ trees.
The optimal number of features is again chosen by evaluating the scores in
\autoref{fig:RandomForest_features}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{build/Random_Forest/Random_Forest_features.pdf}
  \caption{Quality parameters of the Random Forest for a varying number of features.}
  \label{fig:RandomForest_features}
\end{figure}

The model stops improving when using more than $60$ features, so this number is chosen for the Random Forest.
As before, the performance of the Random Forest is determined via k-folding the training data
with five splits, the results are

\begin{align*}
  \text{Accuracy} &= 0.9731\pm0.0014 \,, \\
  \text{Precision} &= 0.9873\pm0.0030 \,, \\
  \text{ROC Score} &= 0.99566\pm0.00027 \,.
\end{align*}

The optimal decision threshold is determined in the same way as before, \autoref{fig:RandomForest_threshold}
shows the $f_\beta$ score and its maximum.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{build/Random_Forest/Random_Forest_threshold.pdf}
  \caption{The $f_\beta$ score of the Random Forest classifier and its maximum value.}
  \label{fig:RandomForest_threshold}
\end{figure}

The resulting best cut $\tau_{c \text{, best}}$ is also given in \autoref{fig:RandomForest_threshold}. The resulting confusion matrix 
when using this threshold is given in \autoref{fig:RandomForest_confusion}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{build/Random_Forest/Random_Forest_confusion.pdf}
  \caption{Confusion matrix for the Random Forest classifier.}
  \label{fig:RandomForest_confusion}
\end{figure}

The results are far better compared with the Naive Bayes approach, almost none of the
$\approx 35.000$ events have been incorrectly labeled as signal.

\subsubsection{Neural Network}

The third model used is a Neural Network in form of a multi-Layer Perceptron classifier.
The Network consists of two hidden layers with sizes $60$ and $10$, respectively.
The best number of features is again chosen according to the results shown in
\autoref{fig:NeuralNetwork_features}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{build/Neural_Network/Neural_Network_features.pdf}
  \caption{Quality parameters of the Neural Network for a varying number of features.}
  \label{fig:NeuralNetwork_features}
\end{figure}

As with the Random Forest, no improvement can be seen after using more than
$60$ features, so again $60$ is chosen as the number of features here.
The results from using k-folding become
\begin{align*}
  \text{Accuracy} &= 0.9711\pm0.0015 \,, \\
  \text{Precision} &= 0.972\pm0.004 \,, \\
  \text{ROC Score} &= 0.99603\pm0.00034 \,.
\end{align*}

\autoref{fig:NeuralNetwork_threshold} shows the $f_\beta$ score and its maximum.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{build/Neural_Network/Neural_Network_threshold.pdf}
  \caption{The $f_\beta$ score of the Neural Network and its maximum value.}
  \label{fig:NeuralNetwork_threshold}
\end{figure}

Since the Neural Network tends to give predictions either close to zero or one,
a very high cut has to be chosen, the $\tau_{c \text{, best}}$ value is given in
\autoref{fig:NeuralNetwork_threshold}.
After applying this cut, the confusion matrix shown in \autoref{fig:NeuralNetwork_confusion}
results.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{build/Neural_Network/Neural_Network_confusion.pdf}
  \caption{Confusion matrix for the Neural Network.}
  \label{fig:NeuralNetwork_confusion}
\end{figure}

Just as with the Random Forest, almost all signal is labeled correctly.
However, a slighlty larger portion of signal events is labeled as background.

\subsection{Classification of test data}

To make a prediction on the provided test data, one of the three models has
to be chosen. The Naive Bayes classifier is ruled out due to its low performance.
When comparing the Random Forest with the Neural Network, it becomes apparent that
the Random Forest performes slightly better based on the confusion matrices.
A comparison of the ROC curves of each classifier is shown in \autoref{fig:roc_curves}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{build/roc_curves.pdf}
  \caption{Comparison of the ROC curves of the three different classifiers.}
  \label{fig:roc_curves}
\end{figure}

While the Random Forest and Neural Network dont show a large difference here, the
lower performance of the Naive Bayes approach is clearly visible.
Consequently, the Random Forest is applied to the test sample, in which it
identifies $1897$ events as signal.